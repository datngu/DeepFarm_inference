#!/usr/bin/env python

import os
import sys
import numpy as np
import pandas as pd
import argparse
import pickle
import tensorflow as tf
from keras.models import load_model
from sklearn.metrics import roc_auc_score, average_precision_score


# Decoding functions
def parse_record(record):
    name_to_features = {
        'seq': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.string),
    }
    return tf.io.parse_single_example(record, name_to_features)

def decode_record(record):
    seq = tf.io.decode_raw(record['seq'], out_type=tf.float16, little_endian=True)
    label = tf.io.decode_raw(record['label'], out_type=tf.int8, little_endian=True)
    seq = tf.reshape(seq, [-1, 4])
    return seq, label

def get_dataset(record_file, num_threads=8, batch=512):
    dataset = tf.data.TFRecordDataset(record_file, num_parallel_reads=num_threads, compression_type='GZIP')
    dataset = dataset.map(parse_record, num_parallel_calls=num_threads)
    dataset = dataset.map(decode_record, num_parallel_calls=num_threads)
    dataset = dataset.shuffle(buffer_size=batch * 10).batch(batch)
    return dataset

# Evaluation function
def get_evals(label, pred, n_out=95):
    evals = {
        'auroc': np.zeros(n_out),
        'aupr': np.zeros(n_out),
    }
    for i in range(n_out):
        try:
            auroc = roc_auc_score(y_true=label[:, i], y_score=pred[:, i])
            aupr = average_precision_score(y_true=label[:, i], y_score=pred[:, i])
            evals['auroc'][i] = auroc
            evals['aupr'][i] = aupr
        except ValueError:
            evals['auroc'][i] = np.nan
            evals['aupr'][i] = np.nan
    return evals

if __name__ == "__main__":

    parser = argparse.ArgumentParser(description = "Model evaluation...)", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--test", nargs='+', help = "testing data *.tfr generated by 'generate_tfr.py", required = True)
    parser.add_argument("--model", help = "trained model (tensorflow, h5 format)", required = True)
    parser.add_argument("--out", default = "model_evaluation", help = "model output files - only best model saved")
    parser.add_argument('--batch_size', type=int, default = 1024, help = 'batch size for training')
    parser.add_argument('--threads', type=int, default = 8, help = 'CPU cores for data pipeline loading')

    args = parser.parse_args()

    model_path = args.model
    test_files = args.test
    out = args.out
    batch_size = args.batch_size
    num_threads = args.threads
    
    # Load model
    model = load_model(model_path)

    # Prepare test data
    test_data = get_dataset(test_files, batch=batch_size, num_threads=num_threads)
    iter_test_data = iter(test_data)

    # Initialize lists to collect predictions and true labels
    predictions = []
    true_labels = []

    # Iterate through test data and collect predictions and labels
    for data, label in iter_test_data:
        preds = model.predict(data)
        predictions.append(preds)
        true_labels.append(label)

    # Concatenate predictions and true labels
    predictions = np.concatenate(predictions, axis=0)
    true_labels = np.concatenate(true_labels, axis=0)

    # Get evaluation metrics
    evals = get_evals(true_labels, predictions, n_out=true_labels.shape[1])
    # Output file names
    out_pkl = out + '.pkl'
    out_csv = out + '.csv'
    # Write results to output files
    try:
        # Save evaluation results as pickle
        with open(out_pkl, 'wb') as pickle_file:
            pickle.dump(evals, pickle_file)
        
        # Convert evaluation results to DataFrame and save to CSV
        df = pd.DataFrame(evals)
        df.to_csv(out_csv, index=False)
        print(f"auroc: {np.nanmean(evals['auroc'])}") 
        print(f"aupr: {np.nanmean(evals['aupr'])}")

    except Exception as e:
        print(f"Error occurred while saving results: {e}")
